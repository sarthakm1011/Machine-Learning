{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-sne generally 99% time used for visualization\n",
    "# variational auto encoder shapes the\n",
    "# GANs create a new model \n",
    "\n",
    "# Occam's Razor (simple > complex)\n",
    "# Upsampling convolution with toward datascience\n",
    "\n",
    "# Overfitting\n",
    "# More flexible the model is more the overfit\n",
    "# Constrained optimization by limiting the range of weights.\n",
    "\n",
    "# REGULARIZATION\n",
    "# -----------------\n",
    "# Model Complexity\n",
    "# Training Schedule\n",
    "# Data\n",
    "# Regularization\n",
    "# Cross-Validation\n",
    "# Ensemble\n",
    "# Normalization\n",
    "\n",
    "# 1) Data\n",
    "# Class Imbalance is problem for the real world.\n",
    "# GANs also used for data generation\n",
    "\n",
    "# 2) Module Comlexity\n",
    "# Linearly seperable data is a simple model\n",
    "\n",
    "# 3) Cross Validation\n",
    "# K-Fold Cross Validation in scikit-Learn\n",
    "# To check that division of validation and training set is good\n",
    "# This is done in the development phase to choose the final model\n",
    "\n",
    "# 4) Ensemble\n",
    "# after finally choosing different models we take the average of these models\n",
    "# Better if models are entirely different\n",
    "\n",
    "# variance: we find a model with less variance so that for small change in input it does not change output by much\n",
    "# Bias: We find a model which is not biased to a specific case \n",
    "# Towards data science regularization\n",
    "\n",
    "# 5) Regularization\n",
    "# convex optimization mein Lagrange multiplier padho \n",
    "# Ridge Regression = L2 (less consistent as gradient has w value)\n",
    "# Lasso Regression = L1 (More consistent)\n",
    "\n",
    "# 6) Dropout\n",
    "# Generally after dropout \n",
    "# if 2/100 nodes only on then 100 C 2 combinations\n",
    "# Multiply the first layer with expectation value to take the effect of each node for different models\n",
    "# Then add these models probability so that same node's values are not taken agian and again\n",
    "# In PyTorch we change model.eval() when giving the model for production\n",
    "\n",
    "# Probability of dropout should be between 10% to 50%\n",
    "\n",
    "# Assignment\n",
    "# First make an nn architecture\n",
    "# Modify gradient and find different values of lambda and find the best values\n",
    "# Try for different functions \n",
    "# y = w0 + w1x + w2(x**2) + w3(x**4)\n",
    "# apply dropout on all NN architectures\n",
    "# apply Batch Normalization on NN architectures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
