{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# why called linear regression?\n",
    "# To find exact values of coefficients w0 and w1 so that error is minimised\n",
    "\n",
    "# SSE = (summation(yhat-y) ** 2) / n\n",
    "# take it's partial derivative\n",
    "# nestrov gradient\n",
    "# optimizer sebastian ruder graient\n",
    "# Adam optimizer works the best in general\n",
    "\n",
    "## pick intial random w \n",
    "# w = [w0\n",
    "#     w1\n",
    "#     w2]\n",
    "\n",
    "# Hypothesis \n",
    "# Have to compute it's accuracy\n",
    "# Make pred yhat = WT * X\n",
    "\n",
    "# Comput Loss = L = 1/2n ()\n",
    "\n",
    "# Compute Gradient\n",
    "\n",
    "# Update W\n",
    "\n",
    "\n",
    "\n",
    "# calculating the accuracy of linear regression is not possible \n",
    "# the value of prediction values will never be exact \n",
    "# hence we calculate the loss error sum rather than accuracy\n",
    "\n",
    "# Linear regression because for every curve we take a parametric form of that in some \\\n",
    "#other axis means with more dimensions \n",
    "\n",
    "# Linear regression can be solved for any complex function just replace with parametric\n",
    "# Linear with respect to parameter not the values of input or x\n",
    "# yhat = w0 + w1*x + w2*(x**2)\n",
    "# yhat = w0 + w1*t1 + w2 * t2\n",
    "\n",
    "# Logistic regression just adds a new dimension for binary classifying \n",
    "# always a binary classification only, for more than one classifier take more than one pairs\n",
    "# logistic = sigmoid = logit\n",
    "# p(class=1) = yhat = sigma(WtX)\n",
    "# P(class=0) = 1- yhat\n",
    "\n",
    "#\n",
    "# cross entropy = negative log likelihood = -plog(q)\n",
    "\n",
    "# sum of probabilities of all these function\n",
    "# (y=0) logP(y=0)\n",
    "# (y=1) logP(y=1)\n",
    "# Bernoulli Trials\n",
    "\n",
    "# neural nets also increases dimension and make them linearly seperable inside it\n",
    "\n",
    "# Assignment\n",
    "# Study the concept of loss function\n",
    "# derivtion logistic regression \n",
    "# solve and compute logistic regression \n",
    "# rewrite the code and do derivation of the loss function\n",
    "# Write sk learn logistic regression\n",
    "# Pytorch install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
